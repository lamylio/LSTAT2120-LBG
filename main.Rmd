---
title: "LSTAT2120 - Linear Project"
author: "Lamy Lionel, Aurèle Bartolomeo, Rémi Gengler"
date: "26/12/2020"
output:
  html_document: 
    number_sections: yes
    toc: yes
    toc_depth: '3'

  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warnings = FALSE)
knitr::opts_chunk$set(eval = TRUE)
```

```{r options}
opt.digits = 3
opt.barcolor = "#000033"
```

```{r utils}
models.comparison = data.frame()

addComparison = function(model, model_name){
  to_combine = data.frame(
      LogLik = ifelse(is.null(logLik(model)[1]), yes=NA, no=round(logLik(model)[1], opt.digits)),
      AIC = round(AIC(model), opt.digits),
      #BIC = round(BIC(model), opt.digits),
      R2 = round(summary(model)$r.squared, opt.digits),
      Adjusted.R2 = round(summary(model)$adj.r.squared, opt.digits),
      Deviance = ifelse(is.null(deviance(model)), yes=NA, no=round(deviance(model), opt.digits)),
      #Deviance.custom = cost_deviance(base_train_freq$Frequency, model$fitted),
      row.names = as.character(model_name)
  )
  rbind(
    models.comparison,
    to_combine
  )
}
```

# Intro and objective

For this project, we will use a dataset that contains all the FIFA 19 player characteristics (FIFA is a football simulation video game, and our dataset come from the 2019's edition). This dataset contains 18059 observations (all the different players created in the game) and 34 variables (characteristics such as Age, Nationality, Wage, Stamina, ...). 

The purpose of our project is to predict the wage of a player based on all his features. This is a very interesting question because it could help club managers to know if some of their players are being underpaid (that would push the player to leave) or overpaid (that could be a threat to club finances). It could also help recruiters to have an idea of the price to pay for a player, depending on the features he is looking for. Finally, it would be a helpful tool for young players, for example to know on which feature they have to work in order to have their wage increased.

For this purpose we will use linear models only. So for a start we will look more precisely at our dataset and see if the classical assumptions for linear models are respected. We will also check for nonlinearity, influential observations, multicollinearity, heteroskedasticity and autocorrelation. All these concepts will be reexplained in the appropriate section. If some hypotheses seem clearly not satisfied, we will then take remedial actions. We will retain different models based on several regression methods seen in course and will compare them based on robust criterions (for example, their ability to predict correctly th value of the wage for a completely new player).

# Data cleaning and separtion of the dataset

We start by doing a complete case analysis (i.e eliminating all observations that contain missing values). We can do that without intoducing any bias in our analysis, because observations containing missing values constitute an extremely small portion of our dataset (48 of 18207 observations, so less than 0.3%). We than improve the usability of the dataset by changing some units. Initially, the variable Nationality (that recorded the nationality of the player) was encoded as the country of the player, but there were then too much levels and some of them seemed to be irrelevant, so we decided to modify it manually and replace these levels by the associated continents.
Finally, we separate randomly 20% of our observations. These observations will be used for prediction, but not for model estimation. 

```{r data_setup}
set.seed(28122020)
original = read.csv("./sources/foot_last.csv", sep=";", encoding = "utf8", skipNul = T, stringsAsFactors = F)
original = na.omit(original)
original = original[-which(original == 0, arr.ind = T)[,1],] # remove rows where value is 0

# Dummies variables
dummies = data.frame(model.matrix(~ Nationality - 1, data=original), 
                     model.matrix(~ Preferred_Foot - 1, data=original))

# Transformations
dataset = within(original, {
  Weight  = round(as.numeric(Weight*0.453592))
  Height = round(sapply(Height, function(x) 
    as.numeric(strsplit(x, "'")[[1]][1])*12 + as.numeric(strsplit(x, "'")[[1]][2])*2.54))
  
  Value = Value_M #* 10^6
  Wage = Wage_K #* 1000
  
  Value_M = NULL
  Wage_K = NULL
  Nationality = NULL
  Preferred_Foot = NULL
})

# dataset.age_cut = within(dataset, {
#   Age = cut(Age, breaks=seq(from=min(Age), to=max(Age), by=6), right=F)
# })


```

# Descriptive analysis

Among the 34 variables, 27 take values in percentage : it mainly concerns the game features such as agility, dribbling, ball control... The other quantitative variables are the age (given in years), the wage (given in Euros), the height (given in centimeters), the weight (given in kilograms) and the value of the player (given in Euros). We should also emphasize the fact that we have two categorical variables called Preferred Foot (with two levels : Left and Right) and Nationality (with five levels : Africa, America, Asia, Europe and Oceania). We can look at the two corrisponding barplot. We see that more than half of the players are europeean, while the 4 other continents don't exceed 25% individually. We also observe that players that are right-footed are almost four times more than left-footed players.

```{r descriptive}
# Histogram of qualitative variable

par(mfrow=c(1,2))
barplot(prop.table(table(original$Nationality)), main="Nationality of the player", col = opt.barcolor, ylab="Percentage",las=2)
barplot(prop.table(table(original$Preferred_Foot)), main="Preferred foot of the player", col = opt.barcolor, ylab="Percentage")
#barplot(prop.table(table(dataset.age_cut$Age)), main="Age of the players by interval", col = opt.barcolor)


# Mean, Sd, Skewness, Kurtosis
library(moments)
library(formattable) # Better print

desc.quantitative = (function(data){
  
  addStats = function(col, name){
    
    return(data.frame(
      Variable=name,
      Mean = round(mean(col, na.rm = T), opt.digits),
      Std.Deviation = round(sd(col), opt.digits),
      Skewness = round(skewness(col), opt.digits),
      Kurtosis = round(kurtosis(col), opt.digits)
      ))
  }
  
  for (c in colnames(data)){
    if (exists("results")){
      results = rbind(results, addStats(data[[c]], c))
    }else{
      results = addStats(data[[c]], c)
    }
  }
  return (results)
   
})(dataset)

desc.quantitative.order = order(as.character(desc.quantitative$Variable))
desc.quantitative = desc.quantitative[desc.quantitative.order,]
row.names(desc.quantitative) = NULL # hide row number

formattable(desc.quantitative,
            align=c("l", rep("c", ncol(desc.quantitative)-1)),
            list(Skewness = formatter("span", style = x ~ style(color= ifelse(abs(x) > 0.5, ifelse(abs(x) >= 2,"red", "orange"), "black")))))

# Boxplot and Histogram

desc.quantitative.boxplots = (function(data){
  
  drawBoxplot = function(co, name){
    par(cex=0.9, mai=c(0.1,0.1,0.2,0.1))
    par(fig=c(0.1, 1, 0.15, 0.95))
    hist(co, axes=T, main=paste0("Distribution of ", name), col = opt.barcolor, xlab="", xaxt="n", ylab="Count")
    par(fig=c(0.1, 1, 0.05, 0.2), new=T)
    boxplot(co, horizontal = T)
  }
  
  sapply(colnames(data), function (x) drawBoxplot(data[[x]], x))
  
})(dataset)
  
```

Now we can take a look at the table that shows some interesting descriptive values about our quantitative variables. More precisely, we have calculated the mean, the standard deviation, the skewness and the kurtosis for each of them. It would be too massive to describe precisely each result of this Table since we have a lot of variables, but we will try to outline the most important ones. 

Let's start with the Age variable. Players age have mean 25, with a 4.7 standard deviation. We see that the variable is skewed right (nothing surprising, some players can still play at 40 but none can play at 5). The kurtosis is a bit negative (recall that in SAS, a normal distribution has kurtosis 0, not 3), that means that the variable is maybe a bit light-tailed. 

Now we can look at our variable of interest, the wage. It seems that it is not at all following a normal distribution. In fact, we have a a skewness of 7.8 which means that it is very right-tailed. Furthermore, it has a kurtosis of 103 so the wage variable is extremely thick in the tails. We can back up this claim by looking at the histogram and boxplot of the wage. This variable is definitely not normal.

Similarly from the wage variable, we see that the value of the player is also right-skewed and very thick in its tails. Again, we can look at the histogral/bowplot figure of the value variable and we confirm this tendency.

Weight and height of the players seem to be normally distributed (maybe a bit right-skewed for the weight) around 181 cm and 75 kg, respectively.

The remaining variables are all about players abilities. Like we have already said, these variables take values between 0 and 100. We will not describe each of them specifically, since they are all quite similar. In general, they have mean between 45 and 65 and standard deviation between 10 and 20. In addition, they have skewness near 0 and kurtosis near 3. 

We now look at the correlation matrix. The aim of this step is to detect high pairwise correlations (and therefore multicollinearity). Note that even if there were no pairwise correlations, it would not mean that there is no multicollinearity. In fact, there can be multicollinearity invisible in the correlation matrix because multicollinearity can come frome linear dependance between 3 or more variables.

```{r full_corrplot}
library(corrplot)
corrplot(cor(dataset), type="upper", order="hclust", tl.col="black", tl.srt=90, tl.cex = 0.7)
```

We have here the upper part of the correlation matrix. We see that all variables in the bottom right have very high pairwise correlations (lots of those correlations are even superior to 0.8). We are not very surprised of that, because we could expect that good players have globally good abilities and conversely, bad players have on average bad abilities. Thus, those variables contains almost the same information as what we already have in the overall variable. Considering this fact, we decided to discard those variables in order to avoid too much multicollinearity problems. 

After this suppression, we see that the following correlation matrix is better, in the sense that we have no longer blocks of high correlated variables. 

```{r min_corrplot}
# Suppression/merge of correlated variables
clean_dataset = dataset[,c(1:5, 8, 19, 22, 24, 26:27, 31, 32:34)]
corrplot(cor(clean_dataset), type="upper", order="hclust", tl.col="black", tl.srt=90, tl.cex = 0.7)
```

Again, this does not mean that we have no multicollinearity at all. We will look at that before model selction in the next section, with more sophisticated tools. 


# Model Selection

We will now start to select variables to include in our model. We will use the two following types of model selction : a stepwise selection using AIC and a LASSO procedure. 

```{r}
library(olsrr)
library(lmtest)

library(MASS)
library(glmnet)

# Splitting into training and validation
tmp = sample(nrow(clean_dataset), round(.80*nrow(clean_dataset)), replace=F)

train = cbind(clean_dataset[tmp,], dummies[tmp,])
valid = cbind(clean_dataset[-tmp,], dummies[-tmp,])

rm(tmp)

formula.complete = as.formula("Wage ~ .")
formula.complete.logy = as.formula("log(Wage) ~ .")

formula.without = as.formula("Wage ~ . - Value")

model.complete = lm(formula.complete, data=train)
model.complete.logy = lm(formula.complete.logy, data=train)

model.without = lm(formula.without, data=train)

models.comparison = addComparison(model.complete, "Complete")
models.comparison = addComparison(model.complete.logy, "Complete log(Y)")
```



## Stepwise Regression using AIC

We will use a stepwise function that finds a model that minimizes AIC, using a step-by-step approach. As a reminder, the AIC, just like the adjusted R-squared, is a measure of the quality of the model that penalizes the addition of new variables. This is what we need in order to do a stepwise selection. The method will check wheter the AIC keeps decreasing while we include more and more variables. 


```{r model_selection stepwise}
# Stepwise AIC 
aic.mass = stepAIC(model.complete, direction = "both", trace = F)
aic.mass

formula.aic = as.formula("Wage ~ Age + Overall + Potential + Weight + HeadingAccuracy + 
    Reactions + Strength + Interceptions + Value + NationalityAmerica")


models.comparison = addComparison(lm(formula.aic, data = train), "Stepwise AIC")

# Stepwise AIC log(Y)
aic.masslogy = stepAIC(model.complete.logy, direction = "both", trace = F)
aic.masslogy

formula.aiclogy = as.formula("log(Wage) ~ Overall + Potential + Weight + HeadingAccuracy + 
    Reactions + Jumping + Strength + Aggression + Composure + 
    Marking + Value + NationalityAfrica + NationalityAsia + NationalityEurope + 
    Preferred_FootLeft")


models.comparison = addComparison(lm(formula.aiclogy, data = train), "Stepwise AIC log(Y)")
```

In the stepwise summary above we see that the selection discards 6 variables : Age, Height, Weight, Jumping

## LASSO Procedure

The LASSO estimator is comprable to the OLS in the sense that the goal is to minimize the sum of squared residuals. But the main difference lies in the fact that it imposes a constraint on the L1 norm of the model parameter \beta.
Indeed, for a certain coefficient t>0 to be determined, we impose that
\[ \sum_{j=1}^{p} |\beta_j| \leq t \].


```{r}
#lasso
lasso.cv = cv.glmnet(x=data.matrix(within(train, {Wage = NULL})), y = train$Wage, family = "gaussian", alpha = 1)
lasso.coef = coef(lasso.cv, s="lambda.min")

formula.lasso = as.formula("Wage ~ Age + Overall + Potential + Height + Weight + HeadingAccuracy + Reactions + Jumping + Strength + Interceptions + Composure + Marking + Value + NationalityAfrica + NationalityAmerica + NationalityEurope + NationalityOceania + Preferred_FootLeft")
model.lasso = lm(formula.lasso, data=train)

models.comparison = addComparison(model.lasso, "Lasso")



#lasso log(Y)
lasso.cvlogy = cv.glmnet(x=data.matrix(within(train, {Wage = NULL})), y = log(train$Wage), family = "gaussian", alpha = 1)
lasso.coeflogy = coef(lasso.cvlogy, s="lambda.min")

formula.lassology = as.formula("log(Wage) ~ Age + Overall + Potential + Height + Weight + HeadingAccuracy + Reactions + Jumping + Strength + Aggression + Composure + Marking + Value + NationalityAfrica + NationalityAmerica +  NationalityAsia + NationalityEurope + NationalityOceania + Preferred_FootLeft + Preferred_FootRight")
model.lassology = lm(formula.lassology, data=train)

models.comparison = addComparison(model.lassology, "Lasso log(Y)")
```

## Final Model
We have now looked at 6 differents models. The complete regression model, the AIC-stepwise regression model and the lasso regression model. For each of them, we have estimated a model of regression against tha wage variable and against the log_wage variable. We have done that because we suspected non linear relations in our model. 
```{r}
models.comparison
```

The table above show a model comparison for those 6 models. We use various criteria such as the loglikelihhod, the AIC, the adjusted R-squared and the deviance. We see that the R-squared is slightly better if we don't take the log of the response variable, but all the other criteria are far much better when we take it. We decide to take as final model the AIC-stepwise model because it has almost as good goodness of fit criteria as the LASSO, but it has only 15 variables (compared to 20 for the LASSO regression).

```{r}
#setting the final model
model = model.lassology
```

# Underlying Hypotheses Testing

We will now check for nonlinearity, outliers (and influential observations), multicollinearity, heteroskedasticity, autocorrelation and normality of the residuals. If some of these hypotheses are not fullfilled, we will try to take remedial actions.

## Nonlinearity

We would like to chek wheter it was a good choice to make a linear model regression. In particular, we would like to check if the regression function is linear. In order to do that, we will look at scatter plots of the residuals $e_i$ against $X_{ij}$, with j = 1, ..., p-1.  

```{r}
for(i in 1:13){
plot(x = train[,i], y=model$residuals)
}
```

We see that almost
## Outliers

We will first check for outliers with respect to X. For that, we calculate the levrages that we can find with the following formula 
\[h_{ii}=(1, X_{i1}, ..., X_{i,p-1})(X'X)^{-1}(1, X_{i1}, ..., X_{i,p-1})'\].
In fact, they are the elements on the diagonal of the rojection matrix $H=X(X'X)^{-1}X'$. Then, we say that the observation $X_i$ is an outlier if $h_{ii} > \frac{2p}{n}$.


Then, we check for outliers with respect to Y (the wage). We say that $Y_i$ is an outlier if 
\[|d_i^*| > t_{n-p-1; 1-\alpha/2}\], where \[d_i^*=e_i \Bigg(\frac{n-p-1}{SSE(1-h_{ii})-e_i^2} \Bigg)^{1/2}\].


We now look at the influential observations for the fitted values. Recall that the i-th observation is influential if $\big|DFFITS_i\big|>2\sqrt{p/n}$, where $DFFITS_i=d_i^*\sqrt{\frac{h_{ii}}{1-h_{ii}}}$. 


In addition to that kind of influential observations, we have to consider the influential observations for the regression coefficients. The i-th observation is influential if $\big|DFBETAS_{k,i}\big|>2/\sqrt{n}$, where $DFBETAS_{k,i}=\frac{\hat{\beta}_k-\hat{\beta}_{k(i)}}{\sqrt{MSE_{(i)}c_k}}$ and $c_k=(X'X)_{kk}^{-1}$.

# Multicollinearity

We will use the Variance Inflation Factors (VIF) to determine if there is multicollinearity problems. Recall that the VIF for $\hat{\beta}_k$ is defined by $VIF_k = \frac{1}{1-R_k^2}$ for $k=1,..., p-1$, where $R_k^2$ is the coefficient of determination of a regression of $X_k$ on $X_1,...,X_{k-1},X_{k+1},...,X_{p-1}$. The decision rule to decide if there is a multicollinearity problem is the following : if either the maximum VIF is higher than 10 or if the average VIF is considerably larger than 1.


# Heteroskedasticity 

We plot the residuals against the fitted values to consider possible heteroskedasticity. If the variance of the residuals is considerably variable, we discard the hypothesis of homoskedasticity, meaning that there is heteroskedasticity. We can also look at the Breusch-Pagan test to detect heteroskedasticity. Note that the null hypothesis of this test states the homoskedasticity whereas in the alternative hypothesis the variance of the residuals is dependent on the observation (i.e. player) considered (equivalent to heteroskedasticity).


# Autocorrelation

Autocorrelation is discussed through the Breusch-Godfrey test. Recall that this test looks at non-autocorrelation ($H_0$) versus autocorrelation ($H_1$). The null hypothesis can be written as $Corr(\varepsilon_t, \varepsilon_{t-k})=0$ for $k=1,...,p$.


# Normality of the residuals

In order to test the normality of the residuals, we can consider a Quantile-Quantile plot of the residuals.


First we need to check for assumptions
Because OLS estimators might not longer be the BLUE


```{r assumptions}
###
# Nonlinearity
###

# Todo ?

###
# Heteroskedasticity
###

# Breusch Pagan : assumes that the error terms are normally distributed
ols_test_breusch_pagan(model.complete, rhs = T, multiple=T) # Rejet de H0
bptest(model.complete, studentize = F) # Rejet de H0

ols_test_score(model.complete, rhs=T)
ols_test_f(model.complete, rhs=T)

# Correction with Boxcox

library(EnvStats)
boxcox(model.complete, optimize = T)


###
# Collinearity
###

ols_vif_tol(model.complete)
#ols_correlations(model.complete.logy)


###
# Residual diagnostics
###

library(tseries)

jarque.test(resid(model.complete))
jarque.bera.test(resid(model.complete))

ols_plot_resid_qq(model.complete)
ols_plot_resid_fit(model.complete)
```

















