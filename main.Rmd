---
title: "LSTAT2120 - Linear Project"
author: "Lamy Lionel, Aurèle Bartolomeo, Rémi Gengler"
date: "26/12/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warnings = FALSE)
knitr::opts_chunk$set(eval = TRUE)
```

```{r}
opt.digits = 3
opt.barcolor = "#000033"
```

## Intro and objective

For this project, we will use a dataset that contains all the FIFA 19 player characteristics (FIFA is a football simulation video game, and our dataset come from the 2019's edition). This dataset contains 18059 observations (all the different players created in the game) and 34 variables (characteristics such as Age, Nationality, Wage, Stamina, ...). 

The purpose of our project is to predict the wage of a player based on all his features. This is a very interesting question because it could help club managers to know if some of their players are being underpaid (that would push the player to leave) or overpaid (that could be a threat to club finances). It could also help recruiters to have an idea of the price to pay for a player, depending on the features he is looking for. Finally, it would be a helpful tool for young players, for example to know on which feature they have to work in order to have their wage increased.

For this purpose we will use linear models only. So for a start we will look more precisely at our dataset and see if the classical assumptions for linear models are respected. We will also check for nonlinearity, influential observations, multicollinearity, heteroskedasticity and autocorrelation. All these concepts will be reexplained in the appropriate section. If some hypotheses seem clearly not satisfied, we will then take remedial actions. We will retain different models based on several regression methods seen in course and will compare them based on robust criterions (for example, their ability to predict correctly th value of the wage for a completely new player).

## Data cleaning and separtion of the dataset

We start by doing a complete case analysis (i.e eliminating all observations that contain missing values). We can do that without intoducing any bias in our analysis, because observations containing missing values constitute an extremely small portion of our dataset (48 of 18207 observations, so less than 0.3%). We than improve the usability of the dataset by changing some units. Initially, the variable Nationality (that recorded the nationality of the player) was encoded as the country of the player, but there were then too much levels and some of them seemed to be irrelevant, so we decided to modify it manually and replace these levels by the associated continents.
Finally, we separate randomly 20% of our observations. These observations will be used for prediction, but not for model estimation. 

```{r}
set.seed(28122020)
original = read.csv("./sources/foot_last.csv", sep=";", encoding = "utf8", skipNul = T, stringsAsFactors = F)
original = na.omit(original)

# Dummies variables
dummies = data.frame(model.matrix(~ Nationality - 1, data=original), 
                     model.matrix(~ Preferred_Foot - 1, data=original))

# Transformations
dataset = within(original, {

  Age = kmeans(Age, centers = 5, nstart = 10)$cluster #cut(Age, breaks=c(min(Age), round(kmeans(Age, centers = 3, nstart = 10)$centers), max(Age)+1), right=F)
    
  Weight  = round(as.numeric(Weight*0.453592))
  Height = round(sapply(Height, function(x) 
    as.numeric(strsplit(x, "'")[[1]][1])*12 + as.numeric(strsplit(x, "'")[[1]][2])*2.54))
  
  Value = Value_M * 10^6
  Wage = Wage_K * 1000
  
  Value_M = NULL
  Wage_K = NULL
  Nationality = NULL
  Preferred_Foot = NULL
})

dataset.age_cut = cut(original$Age, breaks=seq(from=min(original$Age), to=max(original$Age)+1, by=6), right=F)

# Splitting into training and validation
tmp = sample(nrow(dataset), round(.80*nrow(dataset)), replace=F)

train = cbind(dataset[tmp,], dummies[tmp,])
valid = cbind(dataset[-tmp,], dummies[-tmp,])

rm(tmp)
```

## Descriptive analysis

Among the 34 variables, 27 take values in percentage : it mainly concerns the game features such as agility, dribbling, ball control... The other quantitative variables are the age (given in years), the wage (given in Euros), the height (given in centimeters), the weight (given in kilograms) and the value of the player (given in Euros). We should also emphasize the fact that we have two categorical variables called Preferred Foot (with two levels : Left and Right) and Nationality (with five levels : Africa, America, Asia, Europe and Oceania). We can look at the two corrisponding barplot. We see that more than half of the players are europeean, while the 4 other continents don't exceed 25% individually. We also observe that players that are right-footed are almost four times more than left-footed players.

```{r}
# Histogram of qualitative variable

par(mfrow=c(1,2))
barplot(prop.table(table(original$Nationality)), main="Nationality of the player", col = opt.barcolor, ylab="Percentage",las=2)
barplot(prop.table(table(original$Preferred_Foot)), main="Preferred foot of the player", col = opt.barcolor, ylab="Percentage")
barplot(prop.table(table(dataset.age_cut)), main="Age of the players by interval", col = opt.barcolor)


# Mean, Sd, Skewness, Kurtosis
library(moments)
library(formattable) # Better print
library(ggplot2)

desc.quantitative = (function(data){
  
  addStats = function(col, name){
    
    return(data.frame(
      Variable=name,
      Mean = round(mean(col, na.rm = T), opt.digits),
      Std.Deviation = round(sd(col), opt.digits),
      Skewness = round(skewness(col), opt.digits),
      Kurtosis = round(kurtosis(col), opt.digits)
      ))
  }
  
  for (c in colnames(data)){
    if (exists("results")){
      results = rbind(results, addStats(data[[c]], c))
    }else{
      results = addStats(data[[c]], c)
    }
  }
  return (results)
   
})(cbind(dataset[0:-1], Age=original$Age))

desc.quantitative.order = order(as.character(desc.quantitative$Variable))
desc.quantitative = desc.quantitative[desc.quantitative.order,]
row.names(desc.quantitative) = NULL # hide row number

formattable(desc.quantitative,
            align=c("l", rep("c", ncol(desc.quantitative)-1)),
            list(Skewness = formatter("span", style = x ~ style(color= ifelse(abs(x) > 0.5, ifelse(abs(x) >= 2,"red", "orange"), "black")))))

# Boxplot and Histogram

desc.quantitative.boxplots = (function(data){
  
  drawBoxplot = function(co, name){
    par(cex=0.9, mai=c(0.1,0.1,0.2,0.1))
    par(fig=c(0.1, 1, 0.15, 0.95))
    hist(co, axes=T, main=paste0("Distribution of ", name), col = opt.barcolor, xlab="", xaxt="n", ylab="Count")
    par(fig=c(0.1, 1, 0.05, 0.2), new=T)
    boxplot(co, horizontal = T)
  }
  
  sapply(colnames(data), function (x) drawBoxplot(data[[x]], x))
  
})(cbind(dataset[0:-1], Age=original$Age))
  
```


Now we can take a look at the table that shows some interesting descriptive values about our quantitative variables. More precisely, we have calculated the mean, the standard deviation, the skewness and the kurtosis for each of them. It would be too massive to describe precisely each result of this Table since we have a lot of variables, but we will try to outline the most important ones. 

Let's start with the Age variable. Players age have mean 25, with a 4.7 standard deviation. We see that the variable is skewed right (nothing surprising, some players can still play at 40 but none can play at 5). The kurtosis is a bit negative (recall that in SAS, a normal distribution has kurtosis 0, not 3), that means that the variable is maybe a bit light-tailed. 

Now we can look at our variable of interest, the wage. It seems that it is not at all following a normal distribution. In fact, we have a a skewness of 7.8 which means that it is very right-tailed. Furthermore, it has a kurtosis of 103 so the wage variable is extremely thick in the tails. We can back up this claim by looking at the histogram and boxplot of the wage. This variable is definitely not normal.

Similarly from the wage variable, we see that the value of the player is also right-skewed and very thick in its tails. Again, we can look at the histogral/bowplot figure of the value variable and we confirm this tendency.

Weight and height of the players seem to be normally distributed (maybe a bit right-skewed for the weight) around 181 cm and 75 kg, respectively.

The remaining variables are all about players abilities. Like we have already said, these variables take values between 0 and 100. We will not describe each of them specifically, since they are all quite similar. In general, they have mean between 45 and 65 and standard deviation between 10 and 20. In addition, they have skewness and kurtosis near 0. 

```{r}
library(corrplot)
corrplot(cor(dataset), type="upper", order="hclust", tl.col="black", tl.srt=90, tl.cex = 0.7)
```

```{r}
#suppression/merge of correlated variables

clean_dataset = dataset[,c(1:5, 8, 19, 22, 24, 26:27, 31, 32:34)]
corrplot(cor(clean_dataset), type="upper", order="hclust", tl.col="black", tl.srt=90, tl.cex = 0.7)
```


First we need to check for assumptions
Because OLS estimators might not longer be the BLUE

```{r}
library(olsrr)
library(lmtest)

formula.complete = as.formula("Wage ~ .")
model.complete = lm(formula.complete, data=train)
```


```{r}
###
# Heteroskedasticity
###

# Breusch Pagan : assumes that the error terms are normally distributed
test.breusch = ols_test_breusch_pagan(model.complete, rhs = T, multiple=T) # Rejet de H0
bptest(model.complete, studentize = F) # Rejet de H0

###
# Collinearity
###

ols_vif_tol(model.complete)


###
# Residual diagnostics
###

ols_plot_resid_qq(model.complete)
ols_plot_resid_fit(model.complete)
```

















