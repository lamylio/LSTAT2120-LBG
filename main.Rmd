---
title: "LSTAT2120 - Linear Project"
author: "Lamy Lionel, Aurèle Bartolomeo, Rémi Gengler"
date: "26/12/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warnings = FALSE)
knitr::opts_chunk$set(eval = TRUE)
```

```{r}
opt.digits = 3
opt.barcolor = "#000033"
```

## Intro and objective

For this project, we will use a dataset that contains all the FIFA 19 player characteristics (FIFA is a football simulation video game, and our dataset come from the 2019's edition). This dataset contains 18059 observations (all the different players created in the game) and 34 variables (characteristics such as Age, Nationality, Wage, Stamina, ...). 

The purpose of our project is to predict the wage of a player based on all his features. This is a very interesting question because it could help club managers to know if some of their players are being underpaid (that would push the player to leave) or overpaid (that could be a threat to club finances). It could also help recruiters to have an idea of the price to pay for a player, depending on the features he is looking for. Finally, it would be a helpful tool for young players, for example to know on which feature they have to work in order to have their wage increased.

For this purpose we will use linear models only. So for a start we will look more precisely at our dataset and see if the classical assumptions for linear models are respected. We will also check for nonlinearity, influential observations, multicollinearity, heteroskedasticity and autocorrelation. All these concepts will be reexplained in the appropriate section. If some hypotheses seem clearly not satisfied, we will then take remedial actions. We will retain different models based on several regression methods seen in course and will compare them based on robust criterions (for example, their ability to predict correctly th value of the wage for a completely new player).

## Data importation and cleaning

```{r}
set.seed(28122020)
original = read.csv("./sources/foot_last.csv", sep=";", encoding = "utf8", skipNul = T, stringsAsFactors = F)
original = na.omit(original)

# Dummies variables
dummies = data.frame(model.matrix(~ Nationality - 1, data=original), 
                     model.matrix(~ Preferred_Foot - 1, data=original))

# Transformations
dataset = within(original, {

  Age = kmeans(Age, centers = 5, nstart = 10)$cluster #cut(Age, breaks=c(min(Age), round(kmeans(Age, centers = 3, nstart = 10)$centers), max(Age)+1), right=F)
    
  Weight  = round(as.numeric(Weight*0.453592))
  Height = round(sapply(Height, function(x) 
    as.numeric(strsplit(x, "'")[[1]][1])*12 + as.numeric(strsplit(x, "'")[[1]][2])*2.54))
  
  Value = Value_M * 10^6
  Wage = Wage_K * 1000
  
  Value_M = NULL
  Wage_K = NULL
  Nationality = NULL
  Preferred_Foot = NULL
})

dataset.age_cut = cut(original$Age, breaks=seq(from=min(original$Age), to=max(original$Age)+1, by=6), right=F)

# Splitting into training and validation
tmp = sample(nrow(dataset), round(.80*nrow(dataset)), replace=F)

train = cbind(dataset[tmp,], dummies[tmp,])
valid = cbind(dataset[-tmp,], dummies[-tmp,])

rm(tmp)
```

## Descriptive analysis

```{r}
# Histogram of qualitative variable

par(mfrow=c(1,2))
barplot(prop.table(table(original$Nationality)), main="Nationality of the player", col = opt.barcolor, ylab="Percentage",las=2)
barplot(prop.table(table(original$Preferred_Foot)), main="Preferred foot of the player", col = opt.barcolor, ylab="Percentage")
barplot(prop.table(table(dataset.age_cut)), main="Age of the players by interval", col = opt.barcolor)


# Mean, Sd, Skewness, Kurtosis
library(moments)
library(formattable) # Better print
library(ggplot2)

desc.quantitative = (function(data){
  
  addStats = function(col, name){
    
    return(data.frame(
      Variable=name,
      Mean = round(mean(col, na.rm = T), opt.digits),
      Std.Deviation = round(sd(col), opt.digits),
      Skewness = round(skewness(col), opt.digits),
      Kurtosis = round(kurtosis(col), opt.digits)
      ))
  }
  
  for (c in colnames(data)){
    if (exists("results")){
      results = rbind(results, addStats(data[[c]], c))
    }else{
      results = addStats(data[[c]], c)
    }
  }
  return (results)
   
})(cbind(dataset[0:-1], Age=original$Age))

desc.quantitative.order = order(as.character(desc.quantitative$Variable))
desc.quantitative = desc.quantitative[desc.quantitative.order,]
row.names(desc.quantitative) = NULL # hide row number

formattable(desc.quantitative,
            align=c("l", rep("c", ncol(desc.quantitative)-1)),
            list(Skewness = formatter("span", style = x ~ style(color= ifelse(abs(x) > 0.5, ifelse(abs(x) >= 2,"red", "orange"), "black")))))

# Boxplot and Histogram

desc.quantitative.boxplots = (function(data){
  
  drawBoxplot = function(co, name){
    par(cex=0.9, mai=c(0.1,0.1,0.2,0.1))
    par(fig=c(0.1, 1, 0.15, 0.95))
    hist(co, axes=T, main=paste0("Distribution of ", name), col = opt.barcolor, xlab="", xaxt="n", ylab="Count")
    par(fig=c(0.1, 1, 0.05, 0.2), new=T)
    boxplot(co, horizontal = T)
  }
  
  sapply(colnames(data), function (x) drawBoxplot(data[[x]], x))
  
})(cbind(dataset[0:-1], Age=original$Age))
  
```

First we need to check for assumptions
Because OLS estimators might not longer be the BLUE

```{r}
library(olsrr)
library(lmtest)

formula.complete = as.formula("Wage ~ .")
model.complete = lm(formula.complete, data=train)
```


```{r}
###
# Heteroskedasticity
###

# Breusch Pagan : assumes that the error terms are normally distributed
test.breusch = ols_test_breusch_pagan(model.complete, rhs = T, multiple=T) # Rejet de H0
bptest(model.complete, studentize = F) # Rejet de H0

###
# Collinearity
###

ols_vif_tol(model.complete)


###
# Residual diagnostics
###

ols_plot_resid_qq(model.complete)
ols_plot_resid_fit(model.complete)
```



