---
title: "Linear Models (LSTAT2120) - Project"
author: "Bartolomeo Aurèle, Gengler Rémi, Lamy Lionel"
date: "January 2021"
output:
  prettydoc::html_pretty:
    css: sources/main.css
    theme: cayman
    highlight: github
    toc: yes
    toc_depth: 3
    fig_caption: yes
    number_sections: no
    fontsize: 10pt
  html_document: default
  rmarkdown::html_document: default
  pdf_document:
    toc: yes
    toc_depth: 3
---

<style>
.main-content, .toc{font-size: 0.8rem;}
.main-content pre, code{
  font-size: 0.6rem !important;
  line-height: 1.2;
}
</style>

```{r global.options, include=F}
knitr::opts_chunk$set(
  cache = TRUE,
  echo = F,
  eval=T,	
  	
  warning=F,	
  message=F,
  
  	
  out.width= "50%",
  fig.align = "center",	
  fig.path = "figs/"
  	
)	
rmarkdown::html_document(df_print = "formattable")
```

```{r options}
opt.digits = 3
opt.barcolor = "blue" #"#0a4d7d"
```

```{r utils}
library("kableExtra")
source("./sources/utils.R")
source("./sources/descriptive.R")
```

<style>
i {text-transform: normal; font-weight: 500;}
</style>

# Introduction and objective

<b><u>Note</u>:</b> all the tables, figures and the code used in this project <a href="https://github.com/lamylio/LSTAT2120-LBG">can be found on github</a>.

For this project, we will use a <a href="https://www.kaggle.com/karangadiya/fifa19">dataset</a> that contains all the FIFA 19 player characteristics (FIFA is a football simulation video game, and our dataset come from the 2019's edition). This dataset contains <code id="code">18059</code> observations (all the different players created in the game) and <code id="code">`r 34`</code> variables (characteristics such as Age, Nationality, Wage, Stamina, ...). 

The purpose of our project is to predict the wage of a player based on all his features. This is a very interesting question because it could help club managers to know if some of their players are being underpaid (that would push the player to leave) or overpaid (that could be a threat to club finances). It could also help recruiters to have an idea of the price to pay for a player, depending on the features he is looking for. Finally, it would be a helpful tool for young players, for example to know on which feature they have to work in order to have their wage increased.

For this purpose we will use linear models only. So for a start we will look more precisely at our dataset and see if the classical assumptions for linear models are respected. That is, we check if 

1. The $X$ matrix of explanatory variables is of full rank (i.e. $rk(X)=p$)
2. $X$ is a fixed matrix
3. The error terms $\epsilon_1, ..., \epsilon_n$ are independent
4. $E[\epsilon] = 0$
5. $Var(\epsilon)=\sigma^2I_n$

We will also check for nonlinearity, influential observations, multicollinearity (this is linked to assumption 1), heteroskedasticity (this is assumption 5) and autocorrelation (this is linked to assumption 3). All these concepts will be reexplained in the appropriate section. If some hypotheses seem clearly not satisfied, we will then take remedial actions. We will retain different models based on several regression methods seen in course and will compare them based on robust criterions (for example, their ability to predict correctly the value of the wage for a completely new player).

# Data cleaning and separtion of the dataset

We start by doing a complete case analysis (i.e. eliminating all observations that contain missing values). We can do that without introducing any bias in our analysis, because observations containing missing values constitute an extremely small portion of our dataset (<code id="code">48 of 18207</code> observations, so less than <code id="code">`r 0.3`%</code>). We then improve the usability of the dataset by changing some units. Initially, the variable <i>Nationality</i> that records the nationality of the player was encoded as the country of the player. Since there were too much levels and some of them seemed to be irrelevant, we decided to modify it manually and replace these levels by the associated continents.
Finally, we separated randomly <code id="code">20%</code> of our observations. These observations will be used for prediction, but not for model estimation. 

```{r data_setup}
set.seed(28122020)
original = read.csv("./sources/foot_last.csv", sep=";", encoding = "utf8", skipNul = T, stringsAsFactors = F)
original = na.omit(original)
original = original[-which(original == 0, arr.ind = T)[,1],] # remove rows where value is 0

# Dummies variables
dummies = data.frame(model.matrix(~ Nationality - 1, data=original), 
                     model.matrix(~ Preferred_Foot - 1, data=original))

# Transformations
dataset = within(original, {
  Weight  = round(as.numeric(Weight*0.453592))
  Height = round(sapply(Height, function(x) 
    (as.numeric(strsplit(x, "'")[[1]][1])*12 + as.numeric(strsplit(x, "'")[[1]][2]))*2.54))
  
  Value = Value_M #* 10^6
  Wage = Wage_K #* 1000
  
  Value_M = NULL
  Wage_K = NULL
  Nationality = NULL
  Preferred_Foot = NULL
})

```

# Descriptive analysis

Among the 34 variables, 27 take values in percentage : it mainly concerns the game features such as agility, dribbling, ball control... The other quantitative variables are the age (given in years), the wage (given in thousands of euros), the height (given in centimeters), the weight (given in kilograms) and the value of the player (given in million of euros). We should also emphasize the fact that we have two categorical variables called <i>Preferred_Foot</i> (with two levels : Left and Right) and <i>Nationality</i> (with five levels : Africa, America, Asia, Europe and Oceania). We can look at the two corresponding barplots.

```{r descriptive}
# Histogram of qualitative variable

par(mfrow=c(1,2))
barplot(prop.table(table(original$Preferred_Foot)), main="Preferred foot of the player", col = opt.barcolor, ylab="Percentage")
barplot(prop.table(table(original$Nationality)), main="Nationality of the player", col = opt.barcolor, ylab="Percentage",las=2)
```

We observe that right-footed players are four times as many as left-footed players. We also see that more than half of the players are European, while the 4 other continents don't exceed 25% individually. 


Now we can take a look at the table below that shows some interesting descriptive values about our quantitative variables. More precisely, we have calculated the mean, the standard deviation, the skewness and the kurtosis for each of them. It would be too massive to describe precisely each result of this table since we have a lot of variables, but we will try to outline the most important ones. 

\begin{center}
```{r descriptive table, out.width="80%"}

desc.quantitative = compute.desc.quantitative(dataset)

formattable(desc.quantitative, align=c("l", rep("c", ncol(desc.quantitative)-1)))
# kable(desc.quantitative)
```
\end{center}


Let's start with the <i>Age</i> variable. Players age have mean <code id="code">25</code>, with a <code id="code">`r 4.7`</code> standard deviation. We see that the variable is right skewed (nothing surprising, some players can still play at 40 but none can play at 5). The kurtosis is slightly less than 3, which means that the variable is maybe a bit light-tailed. 

Now we can look at our variable of interest, the wage. It seems that it is not at all following a normal distribution. In fact, we have a a skewness of <code id="code">`r 7.9`</code> which means that it is very right-tailed. Furthermore, it has a kurtosis of <code id="code">102</code> so the <i>Wage</i> variable is extremely thick in the tails. We can back up this claim by looking at the histogram and boxplot of the wage. This variable is definitely not normal.

```{r descriptive_graph, out.width="100%", fig.height=1.8}
# Boxplot and Histogram

draw.hist.boxplots(
  data.frame(Wage=dataset$Wage, Value=dataset$Value,  Wage.log=log(dataset$Wage), Value.log=log(dataset$Value)),
  1, 4, TRUE
)
  
```

Similarly from the <i>Wage</i> variable, we see in the table that the value of the player is also right-skewed and very thick in its tails. The histogram/boxplot figure of the <i>Value</i> variable above confirms this tendency.

We can see in the histograms above that, if we take the log of these two variables (<i>Wage</i> and <i>Value</i>), we obtain much more suitable distributions. The <Wage> distribution is less agglutinated near zero, and the <Value> distribution seems even normally distributed.

Weight and height of the players seem to be normally distributed (maybe a bit right-skewed for the weight) around <code id="code">`r round(mean(dataset$Weight))`kg</code> and <code id="code">`r round(mean(dataset$Height))`cm</code>, respectively.

The remaining variables are all about players abilities. Like we have already said, these variables take values between 0 and 100. We will not describe each of them specifically, since they are all quite similar. In general, they have mean between 45 and 65 and standard deviation between 10 and 20. In addition, they have skewness near 0 and kurtosis near 3.

We now look at the correlation matrix. The aim of this step is to detect high pairwise correlations (and therefore multicollinearity). Note that even if there were no pairwise correlations, it would not mean that there is no multicollinearity. In fact, there can be multicollinearity invisible in the correlation matrix because multicollinearity can come frome linear dependance between 3 or more variables.

```{r full_corrplot}
library(corrplot)
corrplot(cor(dataset), type="upper", order="hclust", tl.col="black", tl.srt=90, tl.cex = 0.7)
```

We have here the upper part of the correlation matrix. We see that we have a triangle of variables that have very high pairwise correlations (lots of those correlations are even superior to <code id="code">`r 0.8`</code>). We are not very surprised of that, because we could expect that good players have globally good abilities and conversely, bad players have on average bad abilities. Thus, those variables contains almost the same information as what we already have in the <i>Overall</i> variable. Considering this fact, we decided to discard those variables in order to avoid too much multicollinearity problems. 


After this suppression, we see that the following correlation matrix is better, in the sense that we have no longer an enormous block of high correlated variables. 

```{r min_corrplot}
# Suppression/merge of correlated variables
clean_dataset = dataset[,c(1:5, 8, 19, 22, 24, 26:27, 31, 32:34)]

# Est-ce qu'on met la Value en log ? 
clean_dataset$Value = log(clean_dataset$Value)

corrplot(cor(clean_dataset), type="upper", order="hclust", tl.col="black", tl.srt=90, tl.cex = 0.7)
```

Again, this does not mean that we have no multicollinearity at all. We will look after the model selection, with more sophisticated tools. 


# Model Selection

We will now start to select variables to include in our model. We will use the two following types of model selection: a stepwise regression using Akaike information criterion (AIC) and a LASSO procedure. 

```{r}
library(olsrr)
library(lmtest)

library(MASS)
library(glmnet)

# Splitting into training and validation
tmp = sample(nrow(clean_dataset), round(.80*nrow(clean_dataset)), replace=F)

train = cbind(clean_dataset[tmp,], dummies[tmp,])
valid = cbind(clean_dataset[-tmp,], dummies[-tmp,])

rm(tmp)


# ---
formula.complete = as.formula("Wage ~ .")
formula.complete.logy = as.formula("log(Wage) ~ .")

model.complete = lm(formula.complete, data=train)
model.complete.logy = lm(formula.complete.logy, data=train)

#models.comparison = addComparison(model.complete, "Complete")
models.comparison = addComparison(model.complete.logy, "Complete log(Y)")
```



## Stepwise Regression using AIC

We will use a stepwise function that finds a model that minimizes AIC, using a step-by-step approach. As a reminder, the AIC, just like the adjusted R-squared, is a measure of the quality of the model that penalizes the addition of new variables. Let $\hat{L}$ be the maximized value of the <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood function</a>, so the lower the AIC, the better the goodness of fit of the model. $$AIC = 2k - 2\log(\hat{L})$$ This is what we need in order to do a stepwise regression. The method will check whether the AIC keeps decreasing while we include or remove more and more variables. This is what we need in order to do a stepwise regression. The method will check whether the AIC keeps decreasing while we include more and more variables. 

```{r model_selection stepwise}
# Stepwise AIC 

model.aic = stepAIC(model.complete, direction = "both", trace = F)
#formula.aic = as.formula(paste0("Wage ~ ", paste(names(aic.mass$coefficients)[-1:0], collapse=" + ")))


# Stepwise AIC log(Y)

model.aiclogy = stepAIC(model.complete.logy, direction = "both", trace = F, k = 2)
#formula.aiclogy = as.formula(paste0("log(Wage) ~ ", paste(names(aic.masslogy$coefficients)[-1:0], collapse=" + ")))


#model.aic = lm(formula.aic, data = train)
#model.aiclogy = lm(formula.aiclogy, data = train)

#models.comparison = addComparison(model.aic, "Stepwise AIC")
models.comparison = addComparison(model.aiclogy, "Stepwise AIC log(Y)")

aicdiff = setdiff(names(train), names(model.aiclogy$coefficients))


```

In the <a href="#stepwise">stepwise summary in the appendix</a> we see that the selection discards `r length(aicdiff)` variables : `r aicdiff`.

## LASSO Procedure

The LASSO estimator is comparable to the OLS in the sense that the goal is to minimize the sum of squared residuals. But the main difference lies in the fact that it imposes a constraint on the L1 norm of the model parameter $\beta$.
Indeed, for a certain coefficient $t>0$ to be determined, we impose that \[ \sum_{j=1}^{p} |\beta_j| \leq t \] We can see the results in the <a href="#lasso">appendix</a>.


```{r}
#lasso
lasso.cv = cv.glmnet(x=data.matrix(within(train, {Wage = NULL})), y = train$Wage, family = "gaussian", alpha=1)
lasso.coef = coef(lasso.cv, 0.5661)

formula.lasso = as.formula(paste0("Wage ~ ", paste(rownames(lasso.coef)[which(abs(lasso.coef) > 0)][0:-1], collapse=" + ")))
model.lasso = lm(formula.lasso, data=train)

#models.comparison = addComparison(model.lasso, "Lasso")


#lasso log(Y)
lasso.cvlogy = cv.glmnet(x=data.matrix(within(train, {Wage = NULL})), y = log(train$Wage), family = "gaussian", alpha = 1)
lasso.coeflogy = coef(lasso.cvlogy, 0.04641)

formula.lassology = as.formula(paste0("log(Wage) ~ ", paste(rownames(lasso.coeflogy)[which(abs(lasso.coeflogy) > 0)][0:-1], collapse=" + ")))
model.lassology = lm(formula.lassology, data=train)

models.comparison = addComparison(model.lassology, "Lasso log(Y)")
```

## Final Model (without interactions)

<!--We have now looked at 6 different models : the complete regression model (two kinds), the AIC-stepwise regression model (two kinds) and the LASSO regression model (two kinds). Indeed, for each of them, we have estimated a model of regression against the <i>Wage</i> variable and against the <i>log_wage</i> variable. We have done that because we suspected non linear relations in our model. -->

\begin{center}
```{r}
# Set the final model (lasso)
choice = 3
model = model.lassology

# Print the table
formattable(models.comparison, list(
  formattable::area(row=choice) ~ formatter("span", style= ~ style(font.weight="bold"))
))

# kable(models.comparison)
```
\end{center}

The table above shows a model comparison for those `r nrow(models.comparison)` models. We use various criteria such as the log-likelihood, the AIC, the adjusted R-squared and the deviance. <!--We see that the adjusted R-squared is slightly better if we don't take the log of the response variable, but all the other criteria are far much better when we take it. --> We decide to take as final model the `r models.comparison[choice,1]` model with log of the <i>Wage</i> variable because it has almost as good goodness of fit criteria as the Complete model, but it has only `r models.comparison$Vars[choice]` variables. The final model is then given by `r paste0(sapply(seq(1, length(model$coefficients)), function (x) return (paste0(round(coef(model)[x], opt.digits), " ", names(coef(model))[x]))), collapse = " + ")`.

<!--\[\log{Wage} = -7.96 + 0.11 Overall + 0.018Potential + 0.0021Weight + 0.005HeadingAccuracy + 0.0034Reactions \\ - 0.0008Jumping - 0.0035Strength + 0.002Aggression + 0.0018Composure - 0.003 Marking + 0.028Value \\ + 0.104NationalityAfrica + 0.284NationalityAsia + 0.117NationalityEurope + 0.021Preferred\_FootLeft\] 

where <i>NationalityAfrica</i>, <i>NationalityAsia</i>, <i>NationalityEurope</i> and <i>Preferred_FootLeft</i> are dummy variables.
-->

## Interactions

We test different possible interactions in the following : we add each time an interaction term in the basis model (final model) and then look at the properties of the new model. The first (1) model with interaction contains an interaction term between the <i>Preferred_FootLeft</i> variable and the <i>Marking</i> variable. We then consider interaction between <i>Preferred_FootLeft</i> and <i>Value</i> (2), interaction between <i>NationalityEurope</i> and <i>Potential</i> (3) and interaction between <i>NationalityAfrica</i> and <i>Value</i> (4).

\begin{center}
```{r}

formula.model = paste0("log(Wage) ~ ", paste(names(coef(model))[0:-1], collapse=" + "))

model.interaction1 = lm(paste0(formula.model, " + Preferred_FootLeft * Marking"), data = train)
model.interaction2 = lm(paste0(formula.model, " + Preferred_FootLeft * Value"), data = train)
model.interaction3 = lm(paste0(formula.model, " + NationalityEurope * Potential"), data = train)
model.interaction4 = lm(paste0(formula.model, " + NationalityAfrica * Value"), data = train)

models.comparison = data.frame()
models.comparison = addComparison(model, "Final Model")
models.comparison = addComparison(model.interaction1, "Model 1")
models.comparison = addComparison(model.interaction2, "Model 2")
models.comparison = addComparison(model.interaction3, "Model 3")
models.comparison = addComparison(model.interaction4, "Model 4")
formattable(models.comparison, align=c("l", "c"))
# kable(models.comparison)
```
\end{center}

In the comparative table above, we see that the interactions we are testing does not improve a lot our criteria of goodness of fit : it is possible to get an AIC and an adjusted R-square slightly better than in the basis model (that's the case when we consider the interaction between <i>NationalityEurope</i> and <i>Potential</i>) but this difference isn't so much considerable. Adjusted R-squared only reaches <code id="code">`r max(models.comparison$Adjusted.R2)`</code> at best instead of <code id="code">`r models.comparison$Adjusted.R2[1]`</code> in the model without interactions and R-squared remains mainly constant for the different tested models. Therefore we will work with the model without any interaction.

# Underlying Hypotheses Testing

We will now check for nonlinearity, outliers and influential observations, multicollinearity, heteroskedasticity, autocorrelation and normality of the residuals. If some of these hypotheses are not fulfilled, we will try to take remedial actions.

## Nonlinearity

We would like to check whether it was a good choice to make a linear model regression. In particular, we would like to check if the regression function is linear. In order to do that, we will look at scatter plots of the residuals $e_i$ against $X_{ij}$, with $j = 1, ..., p-1$.  

```{r, fig.height=3, out.width="100%"}
#train$Value = log(train$Value)
par(mfrow=c(2,4))
for(i in 2:6){
  par(mar=c(2,4,2,1))
  plot(x = unlist(model$model[i]), y=model$residuals, main = names(coef(model))[i], ylab = "Residuals", pch=16, col=opt.barcolor)
}

```

We are looking for non-linear patterns on the above plots of the residuals and, if necessary, take remedial actions to cope with that problem. For example, if we see a quadratic scatterplot, we would be tempted to add a quadratic term for this explaining variable. In our scatterplots, we don't see evidence of hidden quadratic relations, although the scatterplots for the <i>Overall</i> and <i>Value</i> variables tend to show that there is probably a non-linear relation between those terms and our variable of interest. We choose to keep going with the initial model, because the non-linear formula seems to be not straightforward, and therefore we fear to complicate too much the model if we try to deal with it. In addition, recall that we have already taken the log of the <i>Wage</i> variable, so we have not so much possibilities remaining.

## Outliers

We will first check for outliers with respect to X. For that, we calculate the leverages that we can find with the following formula 
\[h_{ii}=(1, X_{i1}, ..., X_{i,p-1})(X'X)^{-1}(1, X_{i1}, ..., X_{i,p-1})'\]
In fact, they are the elements on the diagonal of the projection matrix $H=X(X'X)^{-1}X'$. Then, we say that the observation $X_i$ is an outlier if $h_{ii} > \frac{2p}{n}$.

```{r}
explanatory = train[, names(coef(model))[0:-1]]

pn = (1+ncol(explanatory))*2/nrow(explanatory)

mafull = as.matrix(explanatory)
macross = crossprod(mafull, mafull)
mainv = solve(macross)

hii = sapply(seq(1, nrow(explanatory)), 
  function(i){
    ma = as.matrix(explanatory[i,])
    return(ma %*% tcrossprod(mainv, ma))
  }
)

outliers_X = length(which(hii > pn))
```

Using the criteria described previously, we obtain <code id="code">`r  outliers_X`</code> outliers with respect to X (about <code id="code">`r round(outliers_X/nrow(explanatory), 3)*100`%</code> of the players). 
Then, we check for outliers with respect to Y (the wage). We say that $Y_i$ is an outlier if \[|d_i^*| > t_{n-p-1; 1-\alpha/2}\] where \[d_i^*=e_i \Bigg(\frac{n-p-1}{SSE(1-h_{ii})-e_i^2} \Bigg)^{1/2}\]

```{r}

rStudentModel = as.matrix(rstudent(model))
quantileStudent = qt(0.975, nrow(explanatory)-ncol(explanatory)-2)

outliers_Y = sum(sapply(seq(1, nrow(explanatory)),
  function(i){
    return(abs(rStudentModel[i]) > quantileStudent)
  }                        
))

```
Again, our decision rule allows us to measure the number of outliers with respect to Y. We have <code id="code">`r outliers_Y`</code> outliers of that kind out of <code id="code">`r nrow(explanatory)`</code> players, which is quite low again (about <code id="code">`r round(outliers_Y/nrow(explanatory), 3)*100`%</code>). Since $t_{14309;0.975} \approx 1.96$, under our criteria, all the outliers with respect to Y are represented in red on the plot in <a href="#outliers-1">appendix</a>. 

We now look at the influential observations for the fitted values. Recall that the i-th observation is influential if $\big|DFFITS_i\big|>2\sqrt{\frac{p}{n}}$, where $DFFITS_i=d_i^*\sqrt{\frac{h_{ii}}{1-h_{ii}}}$. 

```{r}

DFFITS = sapply(seq(1, nrow(explanatory)), 
  function(i){
    return(rStudentModel[i] * sqrt(hii[i]/(1-hii[i])))
  }
)

influential_fitted = length(which(abs(DFFITS) > 2*sqrt((1+ncol(explanatory))/nrow(explanatory))))

p = ols_plot_dffits(model, print_plot = F)

removeBackground(changePlot(p$plot))
```
We only have <code id="code">`r influential_fitted`</code> influential observations for the fitted values : that's not much again (less than 4% of the players). Since $2\sqrt{\frac{p}{n}} \approx 0.07$ in our case, under our criteria, all the influential observations are the observations that exceed the thresholds on the previous plot.


In addition to that kind of influential observations, we have to consider the influential observations for the regression coefficients. 


The decision rule seen in class asserts that the i-th observation is influential if : 

$$ \big|DFBETAS_{k,i}\big|>\frac{2}{\sqrt{n}}, \text{where} \\
 DFBETAS_{k,i}=\frac{\hat{\beta}_k-\hat{\beta}_{k(i)}}{\sqrt{MSE_{(i)}c_k}} \; \text{and} \; c_k=(X'X)_{kk}^{-1}$$ 

In our case, $\frac{2}{\sqrt{n}} \approx 0.02$. Thus, every plot of $DFBETAS$ in appendix corresponds to the coefficient associated with the concerned variable. Then we see that there aren't much influential points for the regression coefficient associated with the <i>Value</i> variable whereas the regression coefficient associated with the <i>Preferred_FootLeft</i> variable has a lot of influential observations.

To have an overview of all the influential points, we look at the Cook's distance. Recall that Cook's distance is defined as $$D_i = \frac{(\hat{\beta}-\hat{\beta}_{(i)})'X'X(\hat{\beta}-\hat{\beta}_{(i)})}{pMSE}=\frac{(\hat{Y}-\hat{Y}_{(i)})'(\hat{Y}-\hat{Y}_{(i)})}{pMSE}$$
Note that $D_i$ measures the influence of the i-th observation on the coefficients and on the fitted values.

```{r}
plot(cooks.distance(model), xlab="Index of the observation", ylab="Cooks distance", pch=16, col = rgb(0,0,255, alpha=125, maxColorValue = 255))
abline(h=4/14326, col="red", lwd=2)
```
Here, the Cook's distances are very low (very close to 0 for the wide majority). We choose as decision rule that the influential points are the ones for which the Cook's distance $D_i$ is bigger than $\frac{4}{n}$ (threshold represented in red on the previous plot) where $n$ is the number of observations (common criteria). 


# Multicollinearity

We will use the Variance Inflation Factors (VIF) to determine if there is multicollinearity problems. Recall that the VIF for $\hat{\beta}_k$ is defined by $$VIF_k = \frac{1}{1-R_k^2} \; \text{for} \; k=1,..., p-1 \; \textrm{where} \; R_k^2$$ is the coefficient of determination of a regression of $X_k$ on $X_1,...,X_{k-1},X_{k+1},...,X_{p-1}$. The decision rule to decide if there is a multicollinearity problem is the following : if either the maximum VIF is higher than <code id="code">10</code> or if the average VIF is considerably larger than <code id="code">1</code> (A $VIF_k$ greater than <code id="code">10</code> means that the corresponding $R^2_k$ is larger than <code id="code">0.9</code>, which is a sign of approximate collinearity). This method, instead of the pairwise correlation matrix that we analyzed before, has the advantage to detect also multicollinearity relations involving more than <code id="code">2</code> variables.

\begin{center}
```{r}
p = ols_vif_tol(model)
p = within(p, {
  Tolerance = round(Tolerance, opt.digits)
  VIF = round(VIF, opt.digits)
})
formattable(p, align=c("l", "c", "c"))
# kable(p)
```
\end{center}


Since we have VIF of <code id="code">`r p[2,3]`</code> (for <i>Overall</i>) and <code id="code">`r p[5,3]`</code> (for <i>Value</i>), based on our decision rule, there is a multicollinearity problem. Moreover, the mean of all these VIF is of <code id="code">`r round(mean(p[,3]), 3)`</code>, which is also considerably larger than than one. Nevertheless, we can not apply Ridge Regression since the underlying assumptions are not satisfied. Indeed, as shown before, the linearity criterion isn't that clear. Moreover, we can show that there is heteroskedasticity (see next section). As a result, we have to cope with that multicollinearity problem in the following.

# Heteroskedasticity 

Based on the scatter plots of the residuals $e_i$ against $X_{ij}$, with $j = 1, ..., p-1$ in the study of nonlinearity, we can strongly suspect that there is heteroskedasticity. Let's have a look at the scatter plot of the residuals against the fitted values. If the variance of the residuals is considerably variable, we discard the hypothesis of homoskedasticity, meaning that there is heteroskedasticity. 

```{r}
#Residuals vs fitted
plot(model$fitted.values, model$residuals, xlab="Fitted values", ylab="Residuals", pch=16, col=opt.barcolor, main = "Residual vs Fitted Values")
abline(h=0, col=2, lwd=2)
```

It seems that the more the fitted values are big, the more our residuals tends to be negative. It is a bit complicated to decide only on basis of this graph whether there is heteroskedasticity, so we decide to do a Breusch-Pagan test. Note that the null hypothesis of this test states the homoskedasticity whereas in the alternative hypothesis the variance of the residuals is dependent on the observation (i.e. player) considered (equivalent to heteroskedasticity).


```{r}
t = ols_test_breusch_pagan(model, rhs = T, multiple=T)
t
```

With a p-value of <code id="code">3.318e-163</code>, it is perfectly clear that we can reject the null hypothesis of homoskedasticity. Hopefully, we have done a robust inference so our $\hat\beta$ estimator is consistent. As a result, we don't really have to consider remedial actions such as the method of weighted least squares and Box-Cox transformation.


# Autocorrelation

Autocorrelation is discussed through the Breusch-Godfrey test. Recall that this test looks at non-autocorrelation ($H_0$) versus autocorrelation ($H_1$). The null hypothesis can be written as $corr(\varepsilon_t, \varepsilon_{t-k})=0$ for $k=1,...,p$.

```{r}
t = bgtest(model, order = 8)
t
```


The p-value of this Breush-Godfrey test tells us that we can not reject the null hypothesis of no autocorrelation since <code id="code">`r round(t$p.value, opt.digits)` > 0.05</code>. Therefore, we don't take remedial actions in this case.




# Normality of the residuals

In order to test the normality of the residuals, we can first consider a Quantile-Quantile plot of the residuals. In this plot, we compare the empirical quantiles of the residuals with those of a normal distribution. 

```{r}
qqnorm(model$residuals, pch = 1, frame = FALSE, col = opt.barcolor)
qqline(model$residuals, lwd = 2)

```

Therefore, the points on this graph should follow the straight line. We see that this is not the case for the first quantiles, even though it seems to get better and better then. We will do a Jarque-Bera test to take the decision of rejecting normality of the residuals or not. This test compares the coefficients of skewness $S$ and kurtosis $\kappa$ with the theoretical values for a normal distribution. Recall that for a normal distribution, skewness is equal to 0 and kurtosis is equal to 3 (this is our null hypothesis $H_0$). The Jarque-Bera test statistic is then given by 
\[JB = \frac{n}{6} \Big[\hat{S^2}+\frac{(\hat\kappa-3)^2}{4}\Big] \sim \chi^2_2\] under $H_0$.

```{r}
jarque.test(model$residuals)
```

With the result of the test above, we reject the normality of the error terms. This could be the result of an inadequate model, in fact this could mean that the error is not random. Like we have already said in the nonlinearity section, a complicated nonlinearity relation could maybe fix this problem, but it would be out of the scope of this project. 



# Significance of the Coefficients
We show below the estimates and significance test for the variables that are included in our model. 


```{r}
summary(model)
```

All our coefficients are significant at an $\alpha$−level of 0.01. We note that all the coefficients estimates $\beta_j$ have a positive sign, apart from <i>NationalityAmerica</i> and the intercept. In fact, beeing American seems to increase your salary by 890\$. Since we are working with the log of the <i>Wage</i>, even a negative coefficient increases the salary. Being Asian, moreover, increases your wage by 1700\$. An explantion for that could be that rising clubs in country like Qatar or Saudi Arabia recruits manly asian players, and pay them well. The explanation for the coefficient of <i>NationalityAmerica</i> could be that South American players are really praised in top Europeean football clubbs.

# Coefficient Linear Combination

We will now test if a linear combination of our coeficients holds. We are interested in the effect of the <i>Overall</i> variable. The objective will be to see if its effect is compensated by the effect of the other coefficients relative to player mindset characteristics. We will therefore test the following hypothesis :

$$H_0 : \beta_{Overall} \\
= \frac{\beta_{Reactions} + \beta_{Composure}}{2}$$

```{r}
library(car)
hyp = c(0, 0, −2, 1, 1, 0, 0, 0)
t = linearHypothesis(model, hypothesis.matrix = hyp)
t
```

We see in the table above that we can reject the restricted model (i.e. the model under $H_0$). We can conclude that the effect of the <i>Overall</i> variable is not equal to the mean of the effect of the players mindset characteristics. 


# Test of nullity of a subset of coefficients

We would like to see if we can simplify our model a bit to a restricted model. For that, we will check if we can say that some of the $\beta_j$ (actually, the ones of <i>Reaction</i> and <i>Composure</i>) are simultaneously equal to 0. We want to test them because we suspect that their role is not crucial to determine the salary of the player.
Our test statistic is 
\[\frac{(SSE_0 - SSE)/q}{SSE/(n-p)} \sim F_{q, n-p} \]
The idea behind this test is that under $H_0$, $SSE$ and $SSE_0$ should be close. We will therefore reject $H_0$ if the statistic is beyond a critical value (for a $F_{q, n-p}$ distribution).

```{r}
t = linearHypothesis(model, c("Reactions=0", "Composure=0"))
t
```

In the result table above, we look at the following null hypothesis :
\[H_0 : \beta_{Reactions}=\beta_{Composure}=0\]

We see that we have a p-value of <code id="code">0.0008</code> for this F-test. Therefore, at our nominal level, we can reject $H_0$. So we reject the assertion that $\beta_{Reactions}$ and $\beta_{Composure}$ are null simultaneously. It seems that the behaviour of the player has then an impact on its wage. Maybe, club managers want players to be exemplary to give a good impression of the club.


# Predictions
We are now interested in calculating confidence intervals for the observations that we left apart in the beginning. Thus, now we are not working with our training dataset anymore (80% of the initial data), but with our validation dataset (the 20% left).

We know that a prediction interval at level ($1-\alpha$)100% for a new vector observation $x_h$ is 
\[\hat{Y}_h \pm t_{n-p;1-\alpha/2}S(1+x_h^{'}(X'X)^{-1}x_h)^{1/2}\]
It is wider than the confidence region for $E[Y_h]$ because for $Y_h$ we also have the incertitude of the error $\epsilon$.


```{r}
predictions = exp(predict(model, newdata = valid, interval = "confidence"))

plot(1:nrow(valid), valid$Wage, pch=16, cex=0.5, xlab="Obs", ylab="Wage", main="Observed vs Predicted", 
     col=rgb(0, 0, 0, alpha=125, max=255))
legend("topright", legend = c("Observed Values", "Predicted Values"), pch = c(16,16), col=c(1,4))
points(1:nrow(valid), predictions[,1], col=opt.barcolor, pch=16, cex=0.5)

aimed = length(which(valid$Wage >= predictions[,2] & valid$Wage <= predictions[,3]))
k=aimed / nrow(valid)*100
```

we calculate that only <code id="code">`r round(k,2)`%</code> of our prediction intervals contains the observed values, which is very poor. However, we can see above the actual values of the <i>Wage</i> variable of the validation dataset, next to the predicted ones. We see a certain fit with the observed data, which is good. 


# Conclusion

Our main goal was to make football managers know the wage they have to pay their players, in order to be around the average of what other clubs do. That is a useful information because it could be a precious help to the club accounting. We started by selecting the variables that would compose our model. We did that using several methods of selection (AIC-stepwise, LASSO). We also checked if we had better goodness of fit criteria by taking the log of the response variable, and that was the case. We selected a model that was not too complicated but that had still a good fit to the training data. It was the LASSO model. We were not surprised to see that the value of the player was significant. In fact, the value of the player is one of the variables that help the most to predict his wage. Unsurprisingly, some game characteristics play a role too. 

We then tested some hypotheses on our selected model. We saw that we have approximate multicollinearity, nonlinearities, heteroskedasticity and non normality of the error term. We tried to take remedial actions against that but we think that the nonlinear relation in the true model is very complicated, since we did not manage to improve the model with simple analytic transformations of our variables. We saw that we have some outliers and influential observations, but not so much in comparison to the size of the dataset. Of course we did not delete them because this phenomenon might hide the lack of an important explanatory variable, such as player popularity, for example. We tried then some linear hypotheses, but we had to reject all of them.

Finally, and maybe the most important part of the project, the prediction. Unfortunately, we have a severe drawback of our model for this section, our model has very bad predictive power. Our prediction intervals only cover the true value observed in the validation dataset only in 3.3% of the cases... 

As a conclusion, we could say that this project was very interesting because it was a dive into a realistic practical problem. 
Furthermore, the fail to predict correctly our response variables with linear models pushes us to learn more about other regression techniques, and therefore diversify our statistics advanced techniques. 



# Appendix

## Histograms and boxplots

```{r, fig.height=1.8, out.width="100%"}
draw.hist.boxplots(data.frame(dataset,  Wage.log=log(dataset$Wage), Value.log=log(dataset$Value)), 1, 4, T)
```

## Model selection
### Stepwise

```{r}
model.aiclogy$anova
```

### LASSO
```{r}
print(model.lassology)
lasso.cvlogy$glmnet.fit
```

## Outliers

```{r}
t = ols_plot_resid_stud_fit(model, print_plot = F)
t$plot$layers[[3]] = NULL;t$plot$layers[[3]] = NULL;
t = removeBackground(t$plot)
plot(t)
```


## Influence diagnostics

```{r, fig.height=3, out.width="80%"}
library(cowplot)
p = ols_plot_dfbetas(model, print_plot = F)

p = lapply(p$plots, function(x) {
  x = removeBackground(changePlot(x))
  x$layers[[2]]$aes_params$size = 1.5
  x$labels$title = substr(x$labels$title, start=27, stop=100)
  return(x)
})

plot_grid(plotlist = p[1:2], ncol = 2)
plot_grid(plotlist = p[3:4], ncol = 2)
plot_grid(plotlist = p[5:6], ncol = 2)
plot_grid(plotlist = p[7:8], ncol = 2)
```


## Code

```{r, eval=F, echo=T}
library(ggplot2)

# ===
# Setup and helper functions


opt.digits = 3
opt.barcolor = "blue"

models.comparison = data.frame()

addComparison = function(model, model_name){
  to_combine = data.frame(
    Model = as.character(model_name),
    Vars = length(names(coef(model)))-1,
    LogLik = ifelse(is.null(logLik(model)[1]), yes=NA, no=round(logLik(model)[1], opt.digits)),
    AIC = round(AIC(model), opt.digits),
    R2 = round(summary(model)$r.squared, opt.digits),
    Adjusted.R2 = round(summary(model)$adj.r.squared, opt.digits),
    Deviance = ifelse(is.null(deviance(model)), yes=NA, no=round(deviance(model), opt.digits))
  )
  rbind(
    models.comparison,
    to_combine
  )
}

# Remove text and change color of ggplot
changePlot = function(p){
  p$layers[[5]] = NULL
  p$layers[[4]] = NULL
  p$layers[[3]]$aes_params$colour = opt.barcolor
  p$layers[[1]]$aes_params$colour = opt.barcolor
  p$layers[[2]]$aes_params$colour = "black"
  p$layers[[3]]$aes_params$shape = 16
  return (p)
}

# Remove background and grid of ggplot
removeBackground = function(p){
  p$theme = list(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank())
  return (p)
}

# Descriptive analysis : Mean, Sd, Skewness and Kurtosis
library(moments)
library(formattable)
compute.desc.quantitative = function(data){
  
  addStats = function(col, name){
    
    return(data.frame(
      Variable=name,
      Mean = round(mean(col, na.rm = T), opt.digits),
      Std.Deviation = round(sd(col), opt.digits),
      Skewness = round(skewness(col), opt.digits),
      Kurtosis = round(kurtosis(col), opt.digits)
    ))
  }
  
  for (c in colnames(data)){
    if (exists("results")){
      results = rbind(results, addStats(data[[c]], c))
    }else{
      results = addStats(data[[c]], c)
    }
  }
  
  results.order = order(as.character(results$Variable))
  results = results[results.order,]
  row.names(results) = NULL
  return (results)
  
}

# Histograms with boxplots below 
draw.hist.boxplots = function(data, nrow=2, ncol=2, boxframe=T){
  
  
  layout(mat=matrix(seq(nrow*ncol*2), 2*nrow, 1*ncol, byrow=F), heights = rep(c(4,1.5), ncol))
  drawPlots = function(coname){
    
    par(mar=c(0, 2, 2, 2))
    hist(data[[coname]], axes=T, main=coname, col = opt.barcolor, xlab="", ylab="Count", xaxt="n")
    par(mar=c(3, 2.3, 0, 2.3))
    boxplot(data[[coname]], horizontal = T, frame=boxframe)
    
  }
  
  invisible(sapply(colnames(data), function (x) drawPlots(x)))
  
}


# ===
# Data importation

set.seed(28122020)
original = read.csv("./sources/foot_last.csv", sep=";", encoding = "utf8", skipNul = T, stringsAsFactors = F)
original = na.omit(original)
original = original[-which(original == 0, arr.ind = T)[,1],] # remove rows where value is 0

# Dummies variables
dummies = data.frame(model.matrix(~ Nationality - 1, data=original), 
                     model.matrix(~ Preferred_Foot - 1, data=original))

# Transformations and cleaning
dataset = within(original, {
  Weight  = round(as.numeric(Weight*0.453592))
  Height = round(sapply(Height, function(x) 
    (as.numeric(strsplit(x, "'")[[1]][1])*12 + as.numeric(strsplit(x, "'")[[1]][2]))*2.54))
  
  Value = Value_M #* 10^6
  Wage = Wage_K #* 1000
  
  Value_M = NULL
  Wage_K = NULL
  Nationality = NULL
  Preferred_Foot = NULL
})

# ===

# Histogram of qualitative variable
par(mfrow=c(1,2))
barplot(prop.table(table(original$Preferred_Foot)), main="Preferred foot of the player", col = opt.barcolor, ylab="Percentage")
barplot(prop.table(table(original$Nationality)), main="Nationality of the player", col = opt.barcolor, ylab="Percentage",las=2)


# Print desc
desc.quantitative = compute.desc.quantitative(dataset)
formattable(desc.quantitative, align=c("l", rep("c", ncol(desc.quantitative)-1)))


# Boxplot and Histogram
draw.hist.boxplots(
  data.frame(Wage=dataset$Wage, Value=dataset$Value,  Wage.log=log(dataset$Wage), Value.log=log(dataset$Value)),
  1, 4, TRUE
)

# ===
# Correlation Matrix 

library(corrplot)
corrplot(cor(dataset), type="upper", order="hclust", tl.col="black", tl.srt=90, tl.cex = 0.7)

# Removing of "similar" variables
clean_dataset = dataset[,c(1:5, 8, 19, 22, 24, 26:27, 31, 32:34)]
clean_dataset$Value = log(clean_dataset$Value)

# New correlation matrix
corrplot(cor(clean_dataset), type="upper", order="hclust", tl.col="black", tl.srt=90, tl.cex = 0.7)

library(olsrr)
library(lmtest)

library(MASS)
library(glmnet)

# ===
# Split into training and validation sets

tmp = sample(nrow(clean_dataset), round(.80*nrow(clean_dataset)), replace=F)

train = cbind(clean_dataset[tmp,], dummies[tmp,])
valid = cbind(clean_dataset[-tmp,], dummies[-tmp,])

rm(tmp)

# ===
# Model selection

# Complete Model 

formula.complete.logy = as.formula("log(Wage) ~ .")
model.complete.logy = lm(formula.complete.logy, data=train)

models.comparison = addComparison(model.complete.logy, "Complete log(Y)")


# Stepwise AIC log(Y) 
model.aiclogy = stepAIC(model.complete.logy, direction = "both", trace = F, k = 2)

models.comparison = addComparison(model.aiclogy, "Stepwise AIC log(Y)")
aicdiff = setdiff(names(train), names(model.aiclogy$coefficients))

# LASSO log(Y) 
lasso.cvlogy = cv.glmnet(x=data.matrix(within(train, {Wage = NULL})), y = log(train$Wage), family = "gaussian", alpha = 1)
lasso.coeflogy = coef(lasso.cvlogy, 0.04641) # Best lambda in our opinion

formula.lassology = as.formula(paste0("log(Wage) ~ ", paste(rownames(lasso.coeflogy)[which(abs(lasso.coeflogy) > 0)][0:-1], collapse=" + ")))

model.lassology = lm(formula.lassology, data=train)
models.comparison = addComparison(model.lassology, "Lasso log(Y)")


# ===
# Chosen model

choice = 3
model = model.lassology

formattable(models.comparison, list(
  formattable::area(row=choice) ~ formatter("span", style= ~ style(font.weight="bold"))
))

formula.model = paste0("log(Wage) ~ ", paste(names(coef(model))[0:-1], collapse=" + "))

# ===
# Variables interactions 

model.interaction1 = lm(paste0(formula.model, " + Preferred_FootLeft * Marking"), data = train)
model.interaction2 = lm(paste0(formula.model, " + Preferred_FootLeft * Value"), data = train)
model.interaction3 = lm(paste0(formula.model, " + NationalityEurope * Potential"), data = train)
model.interaction4 = lm(paste0(formula.model, " + NationalityAfrica * Value"), data = train)

# Reset the model comparison df
models.comparison = data.frame()
models.comparison = addComparison(model, "Final Model")
models.comparison = addComparison(model.interaction1, "Model 1")
models.comparison = addComparison(model.interaction2, "Model 2")
models.comparison = addComparison(model.interaction3, "Model 3")
models.comparison = addComparison(model.interaction4, "Model 4")

formattable(models.comparison, align=c("l", "c"))

# ===
# Non-linearity relations

par(mfrow=c(2,4))
for(i in 2:6){
  par(mar=c(2,4,2,1))
  plot(x = unlist(model$model[i]), y=model$residuals, main = names(coef(model))[i], ylab = "Residuals", pch=16, col=opt.barcolor)
}

# Outliers with respect to X 

explanatory = train[, names(coef(model))[0:-1]]
pn = (1+ncol(explanatory))*2/nrow(explanatory)

# Matrix computation
mafull = as.matrix(explanatory)
macross = crossprod(mafull, mafull)
mainv = solve(macross)

# Use sapply for speed improvement
hii = sapply(seq(1, nrow(explanatory)), 
  function(i){
    ma = as.matrix(explanatory[i,])
    return(ma %*% tcrossprod(mainv, ma))
  }
)

outliers_X = length(which(hii > pn))


# Outliers with respect to Y 

rStudentModel = as.matrix(rstudent(model))
quantileStudent = qt(0.975, nrow(explanatory)-ncol(explanatory)-2)

outliers_Y = sum(sapply(seq(1, nrow(explanatory)),
  function(i){
    return(abs(rStudentModel[i]) > quantileStudent)
  }                        
))


# Influential Obsevations 

DFFITS = sapply(seq(1, nrow(explanatory)), 
  function(i){
    return(rStudentModel[i] * sqrt(hii[i]/(1-hii[i])))
  }
)

influential_fitted = length(which(abs(DFFITS) > 2*sqrt((1+ncol(explanatory))/nrow(explanatory))))

# Plot influential
p = ols_plot_dffits(model, print_plot = F)
removeBackground(changePlot(p$plot))

plot(cooks.distance(model), xlab="Index of the observation", ylab="Cooks distance", pch=16, 
     col = rgb(0,0,255, alpha=125, maxColorValue = 255))
abline(h=4/14326, col="red", lwd=2)

# ===
# Multicollinearity 

p = ols_vif_tol(model)
p = within(p, {
  Tolerance = round(Tolerance, opt.digits)
  VIF = round(VIF, opt.digits)
})
formattable(p, align=c("l", "c", "c"))

# ===
# Heteroskedasticity : Residuals vs fitted 
plot(model$fitted.values, model$residuals, xlab="Fitted values", ylab="Residuals", pch=16, 
     col=opt.barcolor, main = "Residual vs Fitted Values")
abline(h=0, col=2, lwd=2)

# Breusch-Pagan test 
t = ols_test_breusch_pagan(model, rhs = T, multiple=T)

# ===
# Autocorrelation : Breusch-Godfrey test 
t = bgtest(model, order = 8)


# ===
# Normality of the error term 

# QQplot
qqnorm(model$residuals, pch = 1, frame = FALSE, col = opt.barcolor)
qqline(model$residuals, lwd = 2)

# Jarque-Bera test
jarque.test(model$residuals)


# ===
# Coefficient Significance 
summary(model)

# Linear Hypothesis 
library(car)
hyp = c(0, 0, −2, 1, 1, 0, 0, 0)
t = linearHypothesis(model, hypothesis.matrix = hyp)

# Nullity of a small subset 
t = linearHypothesis(model, c("Reactions=0", "Composure=0"))


# ===
# Predictions 

predictions = exp(predict(model, newdata = valid, interval = "confidence"))
plot(1:nrow(valid), valid$Wage, pch=16, cex=0.5, xlab="Obs", ylab="Wage", main="Observed vs Predicted", 
     col=rgb(0, 0, 0, alpha=125, max=255))
points(1:nrow(valid), predictions[,1], col=opt.barcolor, pch=16, cex=0.5)
legend("topright", legend = c("Observed Values", "Predicted Values"), pch = c(16,16), col=c(1,4))

# How many obs are in the CI
aimed = length(which(valid$Wage >= predictions[,2] & valid$Wage <= predictions[,3]))
k=aimed / nrow(valid)*100


# ===
# Appendix 

draw.hist.boxplots(data.frame(dataset,  Wage.log=log(dataset$Wage), Value.log=log(dataset$Value)), 1, 4, T)

model.aiclogy$anova

print(model.lassology)
lasso.cvlogy$glmnet.fit

t = ols_plot_resid_stud_fit(model, print_plot = F)
t$plot$layers[[3]] = NULL;t$plot$layers[[3]] = NULL;
t = removeBackground(t$plot)
plot(t)

library(cowplot)
p = ols_plot_dfbetas(model, print_plot = F)
p = lapply(p$plots, function(x) {
  x = removeBackground(changePlot(x))
  x$layers[[2]]$aes_params$size = 1.5
  x$labels$title = substr(x$labels$title, start=27, stop=100)
  return(x)
})
plot_grid(plotlist = p, ncol = 2)

# ===
# END
```